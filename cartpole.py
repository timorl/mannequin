#!/usr/bin/env python3

import os
import sys
import numpy as np

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
if "DEBUG" in os.environ:
    import IPython.core.ultratb
    sys.excepthook = IPython.core.ultratb.FormattedTB(call_pdb=True)

from worlds import Gym, StochasticPolicy
from models import Input, Layer, Softmax, LReLU
from trajectories import normalize, discount, policy_gradient, print_reward, get_rewards
from optimizers import Adams

def run():
    model = Input(4)
    model = Layer(model, 64)
    model = LReLU(model)
    model = Layer(model, 2)
    model = Softmax(model)

    world = StochasticPolicy(Gym("CartPole-v1"))

    opt = Adams(
        np.random.randn(model.n_params) * 0.1,
        lr=0.0001,
        memory=0.8
    )

    while True:
        model.load_params(opt.get_value())

        trajs = world.trajectories(model, 16)
        print_reward(trajs, max_value=500)

        if np.mean(get_rewards(trajs, episode=np.sum)) >= 498:
            world.render(model)
            return

        trajs = discount(trajs, horizon=500)
        trajs = normalize(trajs)

        grad = policy_gradient(trajs, policy=model)
        opt.apply_gradient(grad)

if __name__ == "__main__":
    run()
